{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh/BiDkWZiYfyHyisfptqP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spazznolo/goalie-consistency/blob/main/dsf_stc_exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "IeItvkY_9xn-"
      },
      "outputs": [],
      "source": [
        "# Load data manipulation modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data visualization modules\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load text manipulation modules\n",
        "import re, string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load machine learning modules\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Load pre-trained word embedding modules\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load datset\n",
        "train = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploring the Dataset**"
      ],
      "metadata": {
        "id": "GZnKvFBQ4pk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outside of the target, which is binary (Disaster or not), all other relevant variables are characters. The main variable is the tweet itself, in the 'text' variable. Outside of that, there is a keyword variable (available for all but 0.8% of tweets) and a location variable (available for 66.7% of tweets)."
      ],
      "metadata": {
        "id": "0QVT3YdAAZUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().mean(axis = 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7VZw92-Anhi",
        "outputId": "1ab85e7c-9c46-42a1-9afe-e1df85e14c8a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id          0.000000\n",
              "keyword     0.008013\n",
              "location    0.332720\n",
              "text        0.000000\n",
              "target      0.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['target'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5oQpZEty-o8",
        "outputId": "97abb829-4316-4afc-a549-3cff36b244ee"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4296597924602653"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop(['target'], axis=1)\n",
        "y = train['target']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=33)"
      ],
      "metadata": {
        "id": "jMs-mjrgX1LI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1: Bag of words model**\n",
        "\n",
        "The first model, which is also the simplest, is built through the text processing method called Bag-of-Words, where the number of occurences of a given word "
      ],
      "metadata": {
        "id": "YppYjmIa46xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign stemmer to object\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Function which preprocesses text in general format for all models\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # removes encoding quirks\n",
        "    text = re.sub(r'&[a-zA-Z]+;?', '', text)\n",
        "    # removes mentions\n",
        "    text = re.sub(r'@[a-zA-Z_]+;?', '', text)\n",
        "    # removes numbers\n",
        "    text = re.sub(r'\\w*\\d+\\w*', '', text)\n",
        "    # make all text lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # tokenizes text\n",
        "    text = nltk.word_tokenize(text)\n",
        "    # removes stop words\n",
        "    text = [i for i in text if i not in stopwords.words('english')]\n",
        "    # removes punctuation\n",
        "    text = [i for i in text if i not in string.punctuation]\n",
        "    # stems alphanumeric values\n",
        "    text = [ps.stem(i) for i in text if i.isalnum() == True]\n",
        "    \n",
        "    return \" \".join(text)\n"
      ],
      "metadata": {
        "id": "YgV8hlt-HhGc"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
        "\n",
        "X_train_vec = vectorizer.fit_transform(X_train['text'].apply(preprocess_text))\n",
        "X_train_vec = pd.DataFrame(X_train_vec.toarray(), columns=vectorizer.get_feature_names_out(), index=X_train.index)\n",
        "\n",
        "X_val_vec = vectorizer.transform(X_val['text'].apply(preprocess_text))\n",
        "X_val_vec = pd.DataFrame(X_val_vec.toarray(), columns=vectorizer.get_feature_names_out(), index=X_val.index)"
      ],
      "metadata": {
        "id": "ED25Jdj4Dg0Q"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of LogisticRegression classifier\n",
        "lr = LogisticRegression(random_state=33)\n",
        "lr.fit(X_train_vec, y_train)\n",
        "y_pred = lr.predict(X_val_vec)\n",
        "  \n",
        "# Use metrics.accuracy_score to measure the score\n",
        "precision = precision_score(y_val, y_pred)\n",
        "fscore = f1_score(y_val, y_pred)\n",
        "accuracy = accuracy_score(y_val, y_pred)"
      ],
      "metadata": {
        "id": "mcMb5OrTOvP9"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision, fscore, accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHRt65mumkz6",
        "outputId": "8a8ff0bf-39da-4dc9-ed19-85e95090de25"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7997159090909091 0.7292746113989638 0.7804621848739496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add plots"
      ],
      "metadata": {
        "id": "h_WEVdN11X9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2: Feature generation and traditional ML model**\n",
        "\n",
        "Now that the basics are covered, we'll test a more modern, complex, and memory-intensive approach. This will be done by first processing the data through a Term Frequency - Inverse Document Frequency (tf-idf) vectorizer."
      ],
      "metadata": {
        "id": "fjer-7XNmfSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the vectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
        "\n",
        "# fit and transform\n",
        "X_train_tfidf = tfidf.fit_transform(X_train['text'].apply(preprocess_text))\n",
        "X_train_tfidf = pd.DataFrame(X_train_tfidf.toarray(), columns = tfidf.get_feature_names_out(), index=X_train.index)\n",
        "\n",
        "# fit and transform\n",
        "X_val_tfidf = tfidf.transform(X_val['text'].apply(preprocess_text))\n",
        "X_val_tfidf = pd.DataFrame(X_val_tfidf.toarray(), columns = tfidf.get_feature_names_out(), index=X_val.index)"
      ],
      "metadata": {
        "id": "xK8OoXp6Uwks"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mining Other Predictors**\n",
        "\n",
        "Twitter is used for a wide variety of purposes. It is used to disseminate official information to the populace, to debate and discuss rapidly evolving news stories, for the creation/dissemination of art, for entertainment among friends. Through all of these use case, the communication tends to have unique stylistic choices. As an example, official information maybe be written more formally, with care taken to follow linguistic conventions, whereas informal communication among friends maybe be shorter and less conventional. It is along these lines of thought that the following predictors were created.\n",
        "\n",
        "\n",
        "*   Tweet length (number of characters in Tweet)\n",
        "*   Average Word Length (not counting stop words)\n",
        "*   Location (binary, location of person when they Tweeted)\n",
        "*   Hashtags (integer, usually used to contribute to a current trend)\n",
        "*   Keyword (binary, seems to be pulled directly from tweet)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ev6SmU54DXEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf['keyword_binary'] = np.where(X_train['keyword'].isnull(), 0, 1)\n",
        "X_val_tfidf['keyword_binary'] = np.where(X_val['keyword'].isnull(), 0, 1)\n",
        "\n",
        "X_train_tfidf['location_binary'] = np.where(X_train['location'].isnull(), 0, 1)\n",
        "X_val_tfidf['location_binary'] = np.where(X_val['location'].isnull(), 0, 1)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "X_train_temp = X_train.copy()\n",
        "X_val_temp = X_val.copy()\n",
        "\n",
        "X_train_temp['length'] = X_train['text'].str.len()\n",
        "X_val_temp['length'] = X_val['text'].str.len()\n",
        "\n",
        "X_train_temp['words'] = X_train['text'].str.count(' ') + 1\n",
        "X_val_temp['words'] = X_val['text'].str.count(' ') + 1\n",
        "\n",
        "X_train_temp['avg_word_length'] = X_train_temp['length']/X_train_temp['words'] \n",
        "X_val_temp['avg_word_length'] = X_val_temp['length']/X_val_temp['words'] \n",
        "\n",
        "X_train_temp['hash_avg'] = X_train['text'].str.count('#')/X_train_temp['words'] \n",
        "X_val_temp['hash_avg'] = X_val['text'].str.count('#')/X_val_temp['words']\n",
        "\n",
        "X_train_temp['mention_avg'] = X_train['text'].str.count('@')/X_train_temp['words'] \n",
        "X_val_temp['mention_avg'] = X_val['text'].str.count('@')/X_val_temp['words']\n",
        "\n",
        "X_train_temp['upper_avg'] = X_train['text'].str.findall(r'[A-Z]').str.len()/X_train_temp['words'] \n",
        "X_val_temp['upper_avg'] = X_val['text'].str.findall(r'[A-Z]').str.len()/X_val_temp['words']\n",
        "\n",
        "X_train_temp['symbol_avg'] = X_train['text'].str.count(r'[^a-zA-Z0-9 ]')/X_train_temp['length'] \n",
        "X_val_temp['symbol_avg'] = X_val['text'].str.count(r'[^a-zA-Z0-9 ]')/X_val_temp['length']\n",
        "\n",
        "X_train_temp['link_count'] = X_train['text'].str.count('http')\n",
        "X_val_temp['link_count'] = X_val['text'].str.count('http')\n",
        "\n",
        "X_train_temp['rt'] = X_train['text'].str.count('RT')\n",
        "X_val_temp['rt'] = X_val['text'].str.count('RT')\n",
        "\n",
        "new_predictors = ['length', 'avg_word_length', 'hash_avg', 'mention_avg', 'upper_avg', 'symbol_avg', 'link_count', 'rt']\n",
        "\n",
        "X_train_temp_scaled = std_scaler.fit_transform(X_train_temp[new_predictors].to_numpy())\n",
        "X_train_temp_scaled = pd.DataFrame(X_train_temp_scaled, \n",
        "                                   columns = new_predictors,\n",
        "                                   index = X_train_tfidf.index)\n",
        "X_val_temp_scaled = std_scaler.fit_transform(X_val_temp[new_predictors].to_numpy())\n",
        "X_val_temp_scaled = pd.DataFrame(X_val_temp_scaled, \n",
        "                                 columns = new_predictors,\n",
        "                                 index = X_val_tfidf.index)\n",
        "\n",
        "X_train_tfidf = pd.concat([X_train_tfidf, X_train_temp_scaled], axis = 1)\n",
        "X_val_tfidf = pd.concat([X_val_tfidf, X_val_temp_scaled], axis = 1)\n"
      ],
      "metadata": {
        "id": "6vOyxx6niwY9"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training and evaluating traditional ML model**\n",
        "Now that the text has been processed using tf-idf, and a handful of other features were derived using our knowledge of Tweets and disasters, we train a traditional ML model with the new model set. Remember, the goal is to determine if a given Tweet is reffering to a real disaster or not - this is a classification problem. Therefore, our choice of learning algorithm is restricted to classifiers."
      ],
      "metadata": {
        "id": "OHaiKaxHB08M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and train traditional ML model\n",
        "clf = LinearSVC(random_state=33, max_iter = 100000)\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Classify observations in the validation set\n",
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "# Calculate performance metrics\n",
        "precision = precision_score(y_val, y_pred)\n",
        "fscore = f1_score(y_val, y_pred)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(precision, fscore, accuracy)"
      ],
      "metadata": {
        "id": "cfHCgKFpXuRi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add plots"
      ],
      "metadata": {
        "id": "v1PfYJ2X1bFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3: Pre-trained word embeddings + linear classifier model**\n",
        "\n",
        "A pre-trained word embedding is .... For this problem, I chose the BERT .... BERT is a leading-edge .... I chose a sentence embedding because we're looking for a single embedding which represents a sequence of word (Tweet). An efficient way to do this is to consider the Tweet as a sentence, and use a sentence embedder. After running the training and validation sets through the pre-trained sentence embedding, there were 768 variables."
      ],
      "metadata": {
        "id": "ZnEMnKjw3RBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained embedding (using sentence because we want a single embedding which represents the sequence of words)\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Encoding train, validation Tweets with pre-trained embedding\n",
        "sentence_embeddings = sbert_model.encode(X_train['text'].apply(preprocess_text).tolist())\n",
        "val_embeddings = sbert_model.encode(X_val['text'].apply(preprocess_text).tolist())"
      ],
      "metadata": {
        "id": "1BJi_32sqyy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building, fitting SGD Classifer to newly created training set\n",
        "clf = make_pipeline(StandardScaler(), SGDClassifier(loss='hinge', max_iter=10000, random_state=33))\n",
        "clf.fit(sentence_embeddings, y_train)\n",
        "\n",
        "# Classifying validation set with newly trained model\n",
        "y_pred = clf.predict(val_embeddings)\n",
        "\n",
        "# Calculating performance metrics\n",
        "precision = precision_score(y_val, y_pred)\n",
        "fscore = f1_score(y_val, y_pred)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(precision, fscore, accuracy)"
      ],
      "metadata": {
        "id": "KN68C1tKs-aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the figure\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "# adjust the height of tLinearSVChe padding between subplots to avoid overlapping\n",
        "plt.subplots_adjust(hspace=0.3)\n",
        "\n",
        "# add a centered suptitle to the figure\n",
        "plt.suptitle(\"Difference in Features, Disaster vs. Non-disaster\", fontsize=20, y=0.91)\n",
        "\n",
        "# add a new subplot iteratively\n",
        "ax = plt.subplot(4, 3, 1)\n",
        "ax = train[train['keyword_binary']==0]['target'].hist(alpha=0.5, label='Non-disaster', bins=40, color='royalblue', density=True)\n",
        "ax = train[train['keyword_binary']==1]['target'].hist(alpha=0.5, label='Disaster', bins=40, color='lightcoral', density=True)\n",
        "\n",
        "# set x_label, y_label, and legend\n",
        "ax.set_xlabel('keyword', fontsize=14)\n",
        "ax.set_ylabel('Probability Density', fontsize=14)\n",
        "ax.legend(loc='upper right', fontsize=14)\n",
        "    \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "maDiC1vayjxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 4 - Recommendations to the clients\n",
        "\n",
        "Create a final plot(s) of the relevant performance metrics from each experiment.\n",
        "\n",
        "Your job is to present this to each client, providing a recommendation to the clients, taking into consideration all of the clients wants and needs.\n",
        "\n",
        "Explain your decisions."
      ],
      "metadata": {
        "id": "iekA53rw2IFM"
      }
    }
  ]
}