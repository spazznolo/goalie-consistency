{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUwWrh7NOt0Rrj0OpnPkMM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spazznolo/goalie-consistency/blob/main/dsf_stc_exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "IeItvkY_9xn-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, string\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "id": "-7k6JZoaAZYN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploring the Dataset**"
      ],
      "metadata": {
        "id": "GZnKvFBQ4pk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outside of the target, which is binary (Disaster or not), all other relevant variables are characters. The main variable is the tweet itself, in the 'text' variable. Outside of that, there is a keyword variable (available for all but 0.8% of tweets) and a location variable (available for 66.7% of tweets)."
      ],
      "metadata": {
        "id": "0QVT3YdAAZUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().mean(axis = 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7VZw92-Anhi",
        "outputId": "bfb61293-30d2-416e-bc3c-29b8b28d602f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id          0.000000\n",
              "keyword     0.008013\n",
              "location    0.332720\n",
              "text        0.000000\n",
              "target      0.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Pre-processing Ideas**\n",
        "\n",
        "It is important to understand the context of the text being analyzed before deciding on the pre-processing steps. This analysis involves public, short-form communication on the popular social media website Twitter through the form of Tweets. There are textual patterns which are particular to Twitter, such as:\n",
        "\n",
        "\n",
        "*   Mentions (@username)\n",
        "*   Hashtags (#subject)\n",
        "*   Retweets \n",
        "\n"
      ],
      "metadata": {
        "id": "fEPhtX_IBuZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop(['target'], axis=1)\n",
        "y = train['target']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=33)\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "jMs-mjrgX1LI"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1: Bag of words model**\n",
        "\n",
        "The first model, which is also the simplest, is built through the text processing method called Bag-of-Words, where the number of occurences of a given word "
      ],
      "metadata": {
        "id": "YppYjmIa46xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function which preprocesses text for bag of words model\n",
        "def preprocess_text(text):\n",
        "    # removes encoding quirks\n",
        "    text = re.sub(r'&[a-zA-Z]+;?', '', text)\n",
        "    # removes mentions\n",
        "    text = re.sub(r'@[a-zA-Z_]+;?', '', text)\n",
        "    # removes mentions\n",
        "    #text = re.sub(r'\\x[a-zA-Z_]+;?', '', text)\n",
        "    # removes numbers\n",
        "    text = re.sub(r'\\w*\\d+\\w*', '', text)\n",
        "    # make all text lowercase\n",
        "    text = text.lower()\n",
        "    # stem text\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "YgV8hlt-HhGc"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
        "\n",
        "X_train_vec = vectorizer.fit_transform(X_train['text'].apply(preprocess_text))\n",
        "X_train_vec = pd.DataFrame(X_train_vec.toarray(), columns=vectorizer.get_feature_names_out(), index=X_train.index)\n",
        "\n",
        "X_val_vec = vectorizer.transform(X_val['text'].apply(preprocess_text))\n",
        "X_val_vec = pd.DataFrame(X_val_vec.toarray(), columns=vectorizer.get_feature_names_out(), index=X_val.index)"
      ],
      "metadata": {
        "id": "ED25Jdj4Dg0Q"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of LogisticRegression classifier\n",
        "lr = LogisticRegression(random_state=33)\n",
        "lr.fit(X_train_vec, y_train)\n",
        "y_pred = lr.predict(X_val_vec)\n",
        "  \n",
        "# Use metrics.accuracy_score to measure the score\n",
        "precision = precision_score(y_val, y_pred)\n",
        "fscore = f1_score(y_val, y_pred)\n",
        "accuracy = accuracy_score(y_val, y_pred)"
      ],
      "metadata": {
        "id": "mcMb5OrTOvP9"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision, fscore, accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHRt65mumkz6",
        "outputId": "a8ec4534-0fa8-4652-c672-33afd7b08c47"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8069164265129684 0.7301173402868317 0.782563025210084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2: Feature generation and traditional ML model**\n",
        "\n",
        "Now that the basics are covered, we'll test a more modern, complex, and memory-intensive approach. This will be done by first processing the data through a Term Frequency - Inverse Document Frequency (tf-idf) vectorizer."
      ],
      "metadata": {
        "id": "fjer-7XNmfSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the vectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
        "\n",
        "# fit and transform\n",
        "X_train_tfidf = tfidf.fit_transform(X_train['text'].apply(preprocess_text))\n",
        "X_train_tfidf = pd.DataFrame(X_train_tfidf.toarray(), columns = tfidf.get_feature_names_out(), index=X_train.index)\n",
        "\n",
        "# fit and transform\n",
        "X_val_tfidf = tfidf.transform(X_val['text'].apply(preprocess_text))\n",
        "X_val_tfidf = pd.DataFrame(X_val_tfidf.toarray(), columns = tfidf.get_feature_names_out(), index=X_val.index)"
      ],
      "metadata": {
        "id": "xK8OoXp6Uwks"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mining Other Predictors**\n",
        "\n",
        "Twitter is used for a wide variety of purposes. It is used to disseminate official information to the populace, to debate and discuss rapidly evolving news stories, for the creation/dissemination of art, for entertainment among friends. Through all of these use case, the communication tends to have unique stylistic choices. As an example, official information maybe be written more formally, with care taken to follow linguistic conventions, whereas informal communication among friends maybe be shorter and less conventional. It is along these lines of thought that the following predictors were created.\n",
        "\n",
        "\n",
        "*   Tweet length (number of characters in Tweet)\n",
        "*   Average Word Length (not counting stop words)\n",
        "*   Location (binary, location of person when they Tweeted)\n",
        "*   Hashtags (integer, usually used to contribute to a current trend)\n",
        "*   Keyword (binary, seems to be pulled directly from tweet)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ev6SmU54DXEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf['keyword_binary'] = np.where(X_train['keyword'].isnull(), 0, 1)\n",
        "X_val_tfidf['keyword_binary'] = np.where(X_val['keyword'].isnull(), 0, 1)\n",
        "\n",
        "X_train_tfidf['location_binary'] = np.where(X_train['location'].isnull(), 0, 1)\n",
        "X_val_tfidf['location_binary'] = np.where(X_val['location'].isnull(), 0, 1)\n",
        "\n",
        "X_train_tfidf['length'] = X_train['text'].str.len()\n",
        "X_val_tfidf['length'] = X_val['text'].str.len()\n",
        "\n",
        "X_train_tfidf['words'] = X_train['text'].str.count(' ') + 1\n",
        "X_val_tfidf['words'] = X_val['text'].str.count(' ') + 1\n",
        "\n",
        "X_train_tfidf['avg_word_length'] = X_train_tfidf['length']/X_train_tfidf['words'] \n",
        "X_val_tfidf['avg_word_length'] = X_val_tfidf['length']/X_val_tfidf['words'] \n",
        "\n",
        "X_train_tfidf['hash_avg'] = X_train['text'].str.count('#')/X_train_tfidf['words'] \n",
        "X_val_tfidf['hash_avg'] = X_val['text'].str.count('#')/X_val_tfidf['words']\n",
        "\n",
        "X_train_tfidf['mention_avg'] = X_train['text'].str.count('@')/X_train_tfidf['words'] \n",
        "X_val_tfidf['mention_avg'] = X_val['text'].str.count('@')/X_val_tfidf['words']\n",
        "\n",
        "X_train_tfidf['upper_avg'] = X_train['text'].str.findall(r'[A-Z]').str.len()/X_train_tfidf['words'] \n",
        "X_val_tfidf['upper_avg'] = X_val['text'].str.findall(r'[A-Z]').str.len()/X_val_tfidf['words']\n",
        "\n",
        "X_train_tfidf['symbol_avg'] = X_train['text'].str.count(r'[^a-zA-Z0-9 ]')/X_train_tfidf['length'] \n",
        "X_val_tfidf['symbol_avg'] = X_val['text'].str.count(r'[^a-zA-Z0-9 ]')/X_val_tfidf['length']\n",
        "\n",
        "X_train_tfidf['http_avg'] = X_train['text'].str.count('http')/X_train_tfidf['words'] \n",
        "X_val_tfidf['http_avg'] = X_val['text'].str.count('http')/X_val_tfidf['words']"
      ],
      "metadata": {
        "id": "6vOyxx6niwY9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf.iloc[:, 45885:45898].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyNSafXYSRhe",
        "outputId": "22006ba5-3acf-465c-c9c1-147146dd25a4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ûótech               0.000048\n",
              "ûótech business      0.000048\n",
              "ûówe                 0.000040\n",
              "ûówe work            0.000040\n",
              "keyword_binary       0.991943\n",
              "location_binary      0.662112\n",
              "length             100.844806\n",
              "avg_word_length      7.098658\n",
              "hash_avg             0.035803\n",
              "mention_avg          0.029172\n",
              "upper_avg            0.742539\n",
              "symbol_avg           0.071311\n",
              "http_avg             0.050235\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training and evaluating traditional ML model**\n",
        "Now that the text has been processed using tf-idf, and a handful of other features were derived using our knowledge of Tweets and disasters, we train a traditional ML model with the new model set. Remember, the goal is to determine if a given Tweet is reffering to a real disaster or not - this is a classification problem. Therefore, our choice of learning algorithm is restricted to classifiers."
      ],
      "metadata": {
        "id": "OHaiKaxHB08M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and train traditional ML model\n",
        "clf = LinearSVC(random_state=33)\n",
        "rf_model = clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Classify observations in the validation set\n",
        "y_pred = rf_model.predict(X_val_tfidf)\n",
        "\n",
        "# Calculate performance metrics\n",
        "precision = precision_score(y_val, y_pred)\n",
        "fscore = f1_score(y_val, y_pred)\n",
        "accuracy = accuracy_score(y_val, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfHCgKFpXuRi",
        "outputId": "b84e7d87-75b9-4098-cfff-0ccf026ebb07"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision, fscore, accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLVYJbUdcrdU",
        "outputId": "012e207a-ca9d-4675-e108-71a64d5c0ec3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9583333333333334 0.10360360360360361 0.5819327731092437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# create the figure\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "# adjust the height of tLinearSVChe padding between subplots to avoid overlapping\n",
        "plt.subplots_adjust(hspace=0.3)\n",
        "\n",
        "# add a centered suptitle to the figure\n",
        "plt.suptitle(\"Difference in Features, Disaster vs. Non-disaster\", fontsize=20, y=0.91)\n",
        "\n",
        "# add a new subplot iteratively\n",
        "ax = plt.subplot(4, 3, 1)\n",
        "ax = train[train['keyword_binary']==0]['target'].hist(alpha=0.5, label='Non-disaster', bins=40, color='royalblue', density=True)\n",
        "ax = train[train['keyword_binary']==1]['target'].hist(alpha=0.5, label='Disaster', bins=40, color='lightcoral', density=True)\n",
        "\n",
        "# set x_label, y_label, and legend\n",
        "ax.set_xlabel('keyword', fontsize=14)\n",
        "ax.set_ylabel('Probability Density', fontsize=14)\n",
        "ax.legend(loc='upper right', fontsize=14)\n",
        "    \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CnfOgSIFGiaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader\n",
        "\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
        "\n",
        "vocab = glove_vectors.vocab.keys()\n",
        "sentence = [\"london fog\", \"is\", \"the\", \"capital\", \"of\", \"great\", \"britain\"]\n",
        "vectors=[]\n",
        "for w in sentence:\n",
        "    if w in vocab:\n",
        "        vectors.append(glove_vectors[w])\n",
        "    else:\n",
        "        print(\"Word {} not in vocab\".format(w))\n",
        "        vectors.append([0])\n",
        "\n",
        "print(vectors)\n",
        "\n",
        "#!pip install sentence_transformers\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "#sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "#sentence_embeddings = sbert_model.encode(X_train['text'].tolist())"
      ],
      "metadata": {
        "id": "oPK8B2hUHo5U"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}